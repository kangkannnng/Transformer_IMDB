{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于Transformer模型的情感分析与情感识别研究\n",
    "## 项目简介\n",
    "本项目通过基于Transformer模型的情感分析和情感识别研究，为社会提供了更准确、高效的情感分析技术。这将在市场调研、社交媒体分析、舆情监测等领域发挥重要作用，帮助企业了解用户情感倾向、产品满意度以及品牌声誉等关键信息，从而支持决策制定和运营优化。\n",
    "\n",
    "同时，对于刚接触神经网络的我来说，这个项目提供了宝贵的机会，能够通过深入研究和实践Transformer模型，提升自然语言处理和深度学习的技能，展示出扎实的研究能力和解决实际问题的能力，并为学术和职业发展奠定坚实基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import collections\n",
    "import os\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torchtext.vocab import vocab\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.model_name = 'Transformer'\n",
    "        # 预训练词向量，此处为None\n",
    "        self.embedding_pretrained = None  \n",
    "        # 测试设备种类，本次实验使用的显卡是RTX1660Ti\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        # 随机失活，防止过拟合\n",
    "        self.dropout = 0.5 \n",
    "        # 类别数，此处为2分类\n",
    "        self.num_classes = 2  \n",
    "        # epoch数，可以根据需要调整\n",
    "        self.num_epochs = 200\n",
    "        # mini-batch大小  \n",
    "        self.batch_size = 20  \n",
    "        # 每句话处理成的长度，短填长切，后面会统一长度\n",
    "        self.pad_size = 500 \n",
    "        # 对读取数据的部分进行赋值，后面会赋值  \n",
    "        self.n_vocab = None\n",
    "        # 学习率\n",
    "        self.learning_rate = 5e-4\n",
    "        # 词向量维度  \n",
    "        self.embed = 300\n",
    "        # 模型的维度，与词向量维度相同\n",
    "        self.dim_model = 300\n",
    "        # 隐藏层大小\n",
    "        self.hidden = 1024\n",
    "        # 最后一个隐藏层大小\n",
    "        self.last_hidden = 512\n",
    "        # 头的数量，这里是5个头，后面可以调整\n",
    "        self.num_head = 5\n",
    "        # 编码器的数量\n",
    "        self.num_encoder = 2\n",
    "        # 保存模型的路径\n",
    "        self.checkpoint_path = './model.ckpt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImdbDataset(Dataset):\n",
    "    # 初始化函数，得到数据并保存到data和labels中\n",
    "    def __init__(\n",
    "        self, folder_path=\"./aclImdb\", is_train=True\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.data, self.labels = self.read_dataset(folder_path, is_train)\n",
    "\n",
    "    # 读取数据，返回数据和标签\n",
    "    def read_dataset(self, folder_path, is_train):\n",
    "        data, labels = [], []\n",
    "        for label in (\"pos\", \"neg\"):\n",
    "            folder_name = os.path.join(\n",
    "                folder_path, \"train\" if is_train else \"test\", label\n",
    "            )\n",
    "            for file in tqdm(os.listdir(folder_name)):\n",
    "                with open(os.path.join(folder_name, file), \"rb\") as f:\n",
    "                    text = f.read().decode(\"utf-8\").replace(\"\\n\", \"\").lower()\n",
    "                    data.append(text)\n",
    "                    labels.append(1 if label == \"pos\" else 0)\n",
    "        return data, labels\n",
    "    \n",
    "    # 返回评论的数量\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # 返回第index条评论和标签\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], int(self.labels[index])\n",
    "\n",
    "    # 返回所有评论\n",
    "    def get_data(self):\n",
    "        return self.data\n",
    "\n",
    "    # 返回所有标签\n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "    \n",
    "\n",
    "# 获取数据集的词元列表，将每条评论转换成词元列表\n",
    "def get_tokenized(data):\n",
    "    def tokenizer(text):\n",
    "        return [tok.lower() for tok in text.split(\" \")]\n",
    "    return [tokenizer(review) for review in data]\n",
    "\n",
    "\n",
    "# 获取数据集的词汇表，将词频低于min_freq的词过滤掉\n",
    "def get_vocab(data):\n",
    "    # 设置词频阈值，低于该阈值的词将被过滤掉\n",
    "    min_freq = 5\n",
    "    # 获取词元列表\n",
    "    tokenized_data = get_tokenized(data)\n",
    "    # 使用Counter统计词频\n",
    "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    # UNK表示未知词元，PAD表示填充词元\n",
    "    vocab_freq = {\"<UNK>\": 0, \"<PAD>\": 1}\n",
    "    # 添加满足词频条件的单词到词汇表，并分配索引\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab_freq[word] = len(vocab_freq)\n",
    "    # 构建词汇表对象并返回\n",
    "    return vocab(vocab_freq)\n",
    "\n",
    "\n",
    "# 数据预处理，将数据转换成神经网络可以接受的输入形式\n",
    "def preprocess_imdb(train_data, vocab, config):\n",
    "    # 将每条评论通过截断或者补0，使得长度变成500\n",
    "    max_l = config.pad_size\n",
    "    # 截短补长\n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x) > max_l else x + [1] * (max_l - len(x))\n",
    "    labels = train_data.get_labels()\n",
    "    tokenized_data = get_tokenized(train_data.get_data())\n",
    "    vocab_dict = vocab.get_stoi()\n",
    "    # 将每条评论转换成词索引\n",
    "    features = torch.tensor(\n",
    "        [pad([vocab_dict.get(word, 0) for word in words]) for words in tokenized_data]\n",
    "    )\n",
    "    labels = torch.tensor([label for label in labels])\n",
    "    # 返回特征和标签\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "# 加载数据集，返回训练集、测试集和词汇表\n",
    "def load_data(config):\n",
    "    train_data = ImdbDataset(folder_path=\"./aclImdb\", is_train=True)\n",
    "    test_data = ImdbDataset(folder_path=\"./aclImdb\", is_train=False)\n",
    "    vocab = get_vocab(train_data.get_data())\n",
    "    train_set = TensorDataset(*preprocess_imdb(train_data, vocab,config))\n",
    "    test_set = TensorDataset(*preprocess_imdb(test_data, vocab,config))\n",
    "    print(f\"训练集大小{train_set.__len__()}\")\n",
    "    print(f\"测试集大小{test_set.__len__()}\")\n",
    "    print(f\"词表中单词个数:{len(vocab)}\")\n",
    "    # 使用DataLoader来创建数据迭代器，在训练中使用迭代器每次产生一个批次的数据\n",
    "    train_iter = DataLoader(\n",
    "        train_set, batch_size=config.batch_size, shuffle=True, num_workers=0\n",
    "    )\n",
    "    test_iter = DataLoader(test_set, config.batch_size)\n",
    "    return train_iter, test_iter, vocab\n",
    "\n",
    "\n",
    "# 预先定义配置\n",
    "config = Config()\n",
    "#加载数据\n",
    "train_data,test_data,vocabs_size = load_data(config)\n",
    "#词表大小额外加上一个PAD，所以加1\n",
    "config.n_vocab = len(vocabs_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现Transformer\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Model, self).__init__()\n",
    "        # 定义词嵌入层，可以将词嵌入层替换成预训练的词向量\n",
    "        if config.embedding_pretrained is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - 1)\n",
    "        # 定义位置编码层\n",
    "        self.postion_embedding = Positional_Encoding(config.embed, config.pad_size, config.dropout, config.device)\n",
    "        # 定义编码器，每个编码器都包含一个多头注意力层和一个前馈神经网络层\n",
    "        self.encoder = Encoder(config.dim_model, config.num_head, config.hidden, config.dropout)\n",
    "        self.encoders = nn.ModuleList([\n",
    "            # 共享参数，复制num_encoder份\n",
    "            copy.deepcopy(self.encoder)\n",
    "            for _ in range(config.num_encoder)])\n",
    "        # 定义全连接层\n",
    "        self.fc1 = nn.Linear(config.pad_size * config.dim_model, config.num_classes)\n",
    "\n",
    "    # 前向传播\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.postion_embedding(out)\n",
    "        for encoder in self.encoders:\n",
    "            out = encoder(out)\n",
    "        # 展开后输入全连接层\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 定义编码器\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim_model, num_head, hidden, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attention = Multi_Head_Attention(dim_model, num_head, dropout)\n",
    "        self.feed_forward = Position_wise_Feed_Forward(dim_model, hidden, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.attention(x)\n",
    "        out = self.feed_forward(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 定义位置编码层\n",
    "class Positional_Encoding(nn.Module):\n",
    "    def __init__(self, embed, pad_size, dropout, device):\n",
    "        super(Positional_Encoding, self).__init__()\n",
    "        self.device = device\n",
    "        self.pe = torch.tensor([[pos / (10000.0 ** (i // 2 * 2.0 / embed)) for i in range(embed)] for pos in range(pad_size)])\n",
    "        self.pe[:, 0::2] = np.sin(self.pe[:, 0::2])\n",
    "        self.pe[:, 1::2] = np.cos(self.pe[:, 1::2])\n",
    "        # 随机失活，防止过拟合\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + nn.Parameter(self.pe, requires_grad=False).to(self.device)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 定义缩放点积注意力层\n",
    "class Scaled_Dot_Product_Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Scaled_Dot_Product_Attention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, scale=None):\n",
    "        attention = torch.matmul(Q, K.permute(0, 2, 1))\n",
    "        if scale:\n",
    "            attention = attention * scale\n",
    "        # 通过softmax函数获取每个词的权重\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        context = torch.matmul(attention, V)\n",
    "        return context\n",
    "\n",
    "\n",
    "# 定义多头注意力层\n",
    "class Multi_Head_Attention(nn.Module):\n",
    "    def __init__(self, dim_model, num_head, dropout=0.0):\n",
    "        super(Multi_Head_Attention, self).__init__()\n",
    "        self.num_head = num_head\n",
    "        assert dim_model % num_head == 0\n",
    "        self.dim_head = dim_model // self.num_head\n",
    "        self.fc_Q = nn.Linear(dim_model, num_head * self.dim_head)\n",
    "        self.fc_K = nn.Linear(dim_model, num_head * self.dim_head)\n",
    "        self.fc_V = nn.Linear(dim_model, num_head * self.dim_head)\n",
    "        self.attention = Scaled_Dot_Product_Attention()\n",
    "        self.fc = nn.Linear(num_head * self.dim_head, dim_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        Q = self.fc_Q(x)\n",
    "        K = self.fc_K(x)\n",
    "        V = self.fc_V(x)\n",
    "        Q = Q.view(batch_size * self.num_head, -1, self.dim_head)\n",
    "        K = K.view(batch_size * self.num_head, -1, self.dim_head)\n",
    "        V = V.view(batch_size * self.num_head, -1, self.dim_head)\n",
    "\n",
    "        # 缩放因子，论文中的dk，目的是为了使得点积不会太大\n",
    "        scale = K.size(-1) ** -0.5  \n",
    "        context = self.attention(Q, K, V, scale)\n",
    "\n",
    "        context = context.view(batch_size, -1, self.dim_head * self.num_head)\n",
    "        out = self.fc(context)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # 残差连接\n",
    "        out = out + x  \n",
    "        out = self.layer_norm(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 定义前馈神经网络层\n",
    "class Position_wise_Feed_Forward(nn.Module):\n",
    "    def __init__(self, dim_model, hidden, dropout=0.0):\n",
    "        super(Position_wise_Feed_Forward, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_model, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, dim_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        # 使用ReLU激活函数\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = out + x\n",
    "        out = self.layer_norm(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 定义训练函数\n",
    "model = Model(config)\n",
    "# 将模型放到GPU上\n",
    "model.cuda()\n",
    "# 定义优化器为Adam\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=config.learning_rate)\n",
    "# 多分类的任务\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# 训练模型\n",
    "batch_size=config.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 记录训练过程的数据\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "best_acc = 0.0\n",
    "# 记录训练过程中的准确率、精确率、召回率和F1值\n",
    "train_precision_values = []\n",
    "train_recall_values = []\n",
    "train_f1_values = []\n",
    "val_precision_values = []\n",
    "val_recall_values = []\n",
    "val_f1_values = []\n",
    "\n",
    "\n",
    "# 开始训练\n",
    "for epoch in range(config.num_epochs):\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    train_preds, train_labels = [], []\n",
    "    val_preds, val_labels = [], []\n",
    "\n",
    "    # 训练模型\n",
    "    model.train()\n",
    "\n",
    "    # 定义进度条\n",
    "    for i,train_idx in enumerate(tqdm(train_data)):\n",
    "        features, labels = train_idx\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(features) \n",
    "        loss = criterion(outputs, labels)\n",
    "        # 反向传播\n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        # 计算训练集的准确率和损失\n",
    "        _, train_pred = torch.max(outputs, 1) \n",
    "        train_acc += (train_pred.detach() == labels.detach()).sum().item()\n",
    "        train_loss += loss.item()\n",
    "        # 记录训练过程中的预测值和标签\n",
    "        train_preds.extend(train_pred.detach().cpu().numpy().tolist())\n",
    "        train_labels.extend(labels.detach().cpu().numpy().tolist())\n",
    "\n",
    "    # 验证模型\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(test_data)):\n",
    "            features, labels = batch\n",
    "            features = features.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels) \n",
    "            _, val_pred = torch.max(outputs, 1) \n",
    "            val_acc += (val_pred.cpu() == labels.cpu()).sum().item()\n",
    "            val_loss += loss.item()\n",
    "            # 记录验证过程中的预测值和标签\n",
    "            val_preds.extend(val_pred.cpu().numpy().tolist())\n",
    "            val_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    epoch_loss_values.append(train_loss/len(train_data))\n",
    "    metric_values.append(val_acc/25000)\n",
    "\n",
    "    # 计算训练过程中的准确率、精确率、召回率和F1值\n",
    "    train_precision = precision_score(train_labels, train_preds, average='macro', zero_division=0)\n",
    "    train_recall = recall_score(train_labels, train_preds, average='macro', zero_division=0)\n",
    "    train_f1 = f1_score(train_labels, train_preds, average='macro', zero_division=0)\n",
    "    train_precision_values.append(train_precision)\n",
    "    train_recall_values.append(train_recall)\n",
    "    train_f1_values.append(train_f1)\n",
    "    # 计算验证过程中的准确率、精确率、召回率和F1值\n",
    "    val_precision = precision_score(val_labels, val_preds, average='macro', zero_division=0)\n",
    "    val_recall = recall_score(val_labels, val_preds, average='macro', zero_division=0)\n",
    "    val_f1 = f1_score(val_labels, val_preds, average='macro', zero_division=0)\n",
    "    val_precision_values.append(val_precision)\n",
    "    val_recall_values.append(val_recall)\n",
    "    val_f1_values.append(val_f1)\n",
    "\n",
    "    print(f'Model Status: [{epoch+1:03d}/{config.num_epochs:03d}] | '\n",
    "      f'Train Acc: {train_acc/25000:3.5f} | '\n",
    "      f'Loss: {train_loss/len(train_data):3.5f} | '\n",
    "      f'Train Precision: {train_precision:3.5f} | '\n",
    "      f'Train Recall: {train_recall:3.5f} | '\n",
    "      f'Train F1 Score: {train_f1:3.5f} | '\n",
    "      f'Val Acc: {val_acc/25000:3.5f} | '\n",
    "      f'Loss: {val_loss/len(test_data):3.5f} | '\n",
    "      f'Val Precision: {val_precision:3.5f} | '\n",
    "      f'Val Recall: {val_recall:3.5f} | '\n",
    "      f'Val F1 Score: {val_f1:3.5f}')\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), config.checkpoint_path)\n",
    "        print(f'saving model with acc {best_acc/25000:.5f}')\n",
    "    \n",
    "\n",
    "# 画出训练过程中的损失曲线以及准确率曲线\n",
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Iteration Average Loss\")\n",
    "x = [ (i + 1) for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.plot(x, y)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [(i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.plot(x, y)\n",
    "plt.savefig('multi_train.jpg')\n",
    "\n",
    "# 画出训练过程中的精确率、召回率和F1分数曲线\n",
    "plt.figure(\"train_metrics\", (18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Train Precision\")\n",
    "x = [(i + 1) for i in range(len(train_precision_values))]\n",
    "y = train_precision_values\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Train Recall\")\n",
    "x = [(i + 1) for i in range(len(train_recall_values))]\n",
    "y = train_recall_values\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Train F1 Score\")\n",
    "x = [(i + 1) for i in range(len(train_f1_values))]\n",
    "y = train_f1_values\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.savefig('multi_train_metrics.jpg')\n",
    "\n",
    "plt.figure(\"val_metrics\", (18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Validation Precision\")\n",
    "x = [(i + 1) for i in range(len(val_precision_values))]\n",
    "y = val_precision_values\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Validation Recall\")\n",
    "x = [(i + 1) for i in range(len(val_recall_values))]\n",
    "y = val_recall_values\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Validation F1 Score\")\n",
    "x = [(i + 1) for i in range(len(val_f1_values))]\n",
    "y = val_f1_values\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.savefig('multi_val_metrics.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
